---
title: "AI Transcription from R using Whisper: Part 2"
description: "Speedup on Windows using WSL2 and CUDA"
author: "Jeffrey Girard"
date: "2024-08-16"
image: whisper.webp
draft: false
categories:
  - teaching
  - audio
  - AI
---

## Introduction

In a [previous blog post](../whisper2024/index.qmd), I discussed using the audio.whisper R package to do local, AI-based audio transcription. It worked well but was prohibitively slow (e.g., ~1 minute to process each second of audio). In this blog post, I will discuss how to achieve considerable speed improvements on Windows through a combination of hardware and software. Parts will be more technical but hang in there and I'll do my best to make it achievable. 

Before we dive into things, I'll provide a brief overview of all the steps. First, we will install the Linux operating system on our Winodws 

## Check for CUDA Support

This post assumes that you are using the Windows operating system and that your computer's graphics card supports [CUDA](https://developer.nvidia.com/cuda-faq). To check that this is the case, first look up your graphics card's model number. An easy way to do this is on Windows 11 is to click on the desktop search bar (bottom-left of the screen next to the windows icon) and type in "Device Manager." Then click the arrow next to "Display adapters" and find your graphics card's model name. On my computer, it says "NVIDIA GeForce RTX 2060." Then go to  [this link](https://developer.nvidia.com/cuda-gpus) and click the "CUDA-Enabled NVIDIA Quadro and NVIDIA RTX" and "CUDA-Enabled GeForce and TITAN Products" blocks to open their accordions. Then search for your graphics card's model number (the left tables are for desktop cards and the right tables are for notebook cards). I found "GeForce RTX 2060" on the list under GeForce and TITAN Products with a compute capability of 7.5. Thus, my card is supported!

## Installing Linux on Windows



### Install dependencies

I assume you already have R (and probably an IDE like RStudio) installed. Open this up and install the development version of the [audio.whisper](https://github.com/bnosac/audio.whisper) package from github.

```{r}
#| eval: false

# Install remotes if you don't have it already
# install.packages("remotes") 

# Install audio.whisper from github
remotes::install_github("bnosac/audio.whisper")
```

### Download whisper model

Load this new package and download one of the whisper models: `"tiny"`, `"base"`, `"small"`, `"medium"`, or `"large-v3"`. Earlier entries on that list are smaller (to download and hold in RAM), faster, and less accurate whereas later entries are larger, slower, and more accurate. There are also English-only versions of all but the large model, which end in `".en"` as in `"base.en"`, and these may be more efficient if you know that all speech will be in English. You can learn more about these models via `?whisper_download_model`. For this tutorial, we will go with the `"base"` model.

```{r}
# Load package from library
library(audio.whisper)
```

```{r}
#| collapse: true
#| message: false

# Download or load from file the desired whisper model
model <- whisper("base")
```


:::{.callout-note}
Note that the larger models may take a while to download, so if you get an error that the download took longer than permitted, you can temporarily allow more time via: `options(timeout = 300)`.
:::

### Transcribe example file

The package comes with an example audio file in the proper format, which contains 11 seconds of a speech by John F. Kennedy Jr. Let's load it from file using `system.file()` and then transcribe it using `predict()`.

```{r}
#| echo: true
#| eval: false

# Construct file path to example audio file in package data
jfk <- system.file(package = "audio.whisper", "samples", "jfk.wav")

# Run English transcription using the downloaded whisper model
out <- predict(model, newdata = jfk, language = "en")

# Print transcript
out$data
```

```{r out}
#| echo: false
#| collapse: true
#| cache: true

# Construct file path to example audio file in package data
jfk <- system.file(package = "audio.whisper", "samples", "jfk.wav")

# Run English transcription using the downloaded whisper model
out <- predict(model, newdata = jfk, language = "en", trace = FALSE)

# Print transcript
out$data |> kableExtra::kbl() |> kableExtra::kable_styling()
```

The results look good! But we can see how long this took by digging into the output object.

```{r}
#| collapse: true

# Examine the time elapsed to process this audio
out$timing
```

Yikes, `r round(as.double(out$timing$transcription_duration))` minutes to process just 11 seconds of audio. That's motivation to work on the CUDA version to speed things up. But before we move on to that, I'll first show you how to extract audio from a video file and convert it to the format that Whisper wants.

### Extract and format audio

Download the example <a href="mlk.mp4" download>mlk.mp4</a> video file, which contains 12 seconds of a speech by Martin Luther King, Jr. This video contains an audio stream in AAC format with a sampling rate of 44.1 kHz. However, whisper requires audio files in WAV format with a sampling rate of 16 kHz. We can extract and convert it in one step using the `av_audio_convert()` function from the `av` package.

```{r convert}
#| collapse: true

# Install av package if you don't have it already
# install.packages("av")

# Load package from library
library(av)

# Extract and convert audio
av_audio_convert(
  "mlk.mp4", 
  output = "mlk.wav", 
  format = "wav", 
  sample_rate = 16000
)
```

Note that the process would have been identical if this had been an audio file in a different format rather than a video file - you would just replace the .mp4 file with the audio file (e.g., .mp3). Now let's transcribe this and verify that our conversion worked.

```{r}
#| echo: true
#| eval: false
#| collapse: true

# Run English transcription using the downloaded whisper model
out2 <- predict(model, newdata = "mlk.wav", language = "en")

# Print transcript
out2$data
```

```{r out2}
#| echo: false
#| cache: true
#| collapse: true

# Run English transcription using the downloaded whisper model
out2 <- predict(model, newdata = "mlk.wav", language = "en", trace = FALSE)

# Print transcript
out2$data |> kableExtra::kbl() |> kableExtra::kable_styling()
```

Not perfect (swapped "that" for "but" and omitted an "and") but pretty good. And this only the base model - it might do better with a larger model, but for time's sake I'll leave that until after we get CUDA working in Part 2.

*Part 2 coming soon!*

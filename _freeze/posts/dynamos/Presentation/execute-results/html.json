{
  "hash": "b6d5f68448be6c1d3b595119ee50c2c1",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: styles.css\n    slide-number: \"c/t\"\n    overview: true\n    progress: true\n    mouse-wheel: true\n    incremental: true\n    lang: \"en\"\n    pagetitle: \"DynAMoS Database Slides\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2023-09-13\"\n---\n\n\n::: {.my-title}\n# DynAMoS Database\n[Dynamic Affective Movie Clip<br />Database for Subjectivity Analysis]{.my-subtitle}\n\n::: {.my-details}\n[ACII 2023 | Girard, Tie, & Liebenthal]{}<br />\n{{< fa link size=xs >}} [<https://dynamos.mgb.org>]{.p90}\n:::\n\n![](movie.svg){.absolute bottom=20 right=0 width=360}\n:::\n\n## Emotion Elicitation using Film\n\n- **Researchers often try to [elicit emotions]{.blue} in participants**\n    + *To study the effects of emotion on other phenomena*\n    + *To identify correlates of and group differences in emotion*\n    + *To detect and quantify affect and affective dysregulation*\n\n- **Affective [movie clips]{.blue} are common for elicitation stimuli**\n    + Expertly crafted by actors, directors, etc.\n    + Multimodal, dynamic, and extended in nature\n\n## Types of Emotion Ratings {.p90}\n\n- **Who is providing the measure of emotion?**\n    + [Self]{.b .blue}-reports of participants' own emotional experience\n    + [Observer]{.b .blue}-perceptions of participants' emotional experience\n\n- **What model of emotion is used to report emotion?**\n    + [Discrete]{.b .blue} choices of emotion categories (e.g, happy, angry)\n    + [Continuous]{.b .blue} scores on emotion dimensions (e.g., valence)\n    \n- **How often are the emotions reported?**\n    + [Holistic]{.b .blue} ratings collected once after the movie clip\n    + [Dynamic]{.b .blue} ratings collected repeatedly during movie clip\n\n\n## Archetypical Study Designs {.p90}\n\n- **Psychology**\n    + Large samples (>50 participants) of holistic self-reports\n    + Focus on explaining differences between participants\n\n- **Affective Computing**\n    + Small samples (<10 participants) of dynamic observer-ratings\n    + Focus on predicting the average perception from video features\n    \n- **A Proposed Hybrid: [Subjectivity Analysis]{.blue}**\n    + Large samples (>50 participants) of dynamic self-reports\n    + Focus on explaining differences between participants\n\n## Subjectivity Analysis\n\nExample Research Questions\n\n- Why do participants experience the same stimuli differently?\n\n- How structured/predictable are these individual differences?\n\n- Are some stimuli (or moments) more subjective than others?\n\n- How well can we predict/explain a stimulus's subjectivity?\n\n- Can subjectivity analysis help us improve **personalization**?\n\n- Can subjectivity analysis help us explain/model **ambiguity**?\n\n## Available Databases\n\n- Affective Movie Clip Databases with Dynamic Self-Reports\n    + [DECAF](https://doi.org/10.1109/TAFFC.2015.2392932): 7 participants watch 36 clips (1--2 min each)\n    + [COGNIMUSE](https://cognimuse.cs.ntua.gr/database): 7 participants watch 7 clips (30 min each)\n\n- Limitations for Subjectivity Analysis\n    + We need more participants for studying subjectivity\n    + We need *intermediate* clip durations (2--10 min)\n    \n- **We designed a new database to meet these needs...**\n\n## DynAMoS Database\n\n::: {.nonincremental}\n- **Movie Clip Selection**\n    + 22 movie clips (1999--2018, 2.2--7.1 min, diverse content)\n\n- **Participant Recruitment**\n    + Healthy community members from *Rally with MGB*\n    + 83 participants (56 female, 26 male)\n    + 18--59 years old (M=28.8, SD=9.9)\n    + 43 White, 22 Asian, 12 Black, 5 Other; 11 Hispanic/Latino\n\n:::\n\n## Procedure\n\n1. Complete demographic **questionnaires**\n\n2. Read brief **text description** of clip's (non-affective) context\n\n3. Watch clip while providing **dynamic affect ratings**\n    + [CARMA](https://carma.jmgirard.com) used to self-report emotional valence at 1 Hz\n\n4. After clip ends, provide **holistic affect ratings**\n    + [S-PANAS](10.1016/S0191-8869(98)00251-7) used to self-report positive and negative affect\n\n5. **Repeat** Steps 2--4 for all clips (across two sessions)\n\n## Quantifying Subjectivity\n\n![](river.png)\n\n::: {.footer}\nEstimates from a Bayesian G study ([github.com/jmgirard/varde](https://github.com/jmgirard/varde))\n:::\n\n## Validation Take-aways {.p90}\n\n- **Dynamic Ratings**\n    - There was a lot of subjectivity (i.e., rater variance)\n    - But the average of all raters' scores was very reliable\n    - Lots of clip-by-rater and second-by-rater effects\n- **Holistic Ratings**\n    - Relatively less subjective than dynamic ratings\n    - Inter-rater reliability was high across all clips\n    - Inter-item reliability was high for *most* clips\n\n::: {.footer}\nSee the database paper or website for details\n:::\n\n## Visualizing many time series\n\n\n::: {.cell hash='Presentation_cache/revealjs/unnamed-chunk-1_857323726b37c3890bf94ed15b821155'}\n::: {.cell-output-display}\n![](Presentation_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n::: {.footer}\nEach blue line is one participant's time series\n:::\n\n## *Chromodoris quadricolor*\n\n![](chromodoris.png)\n\n## Introducing the Chromodoris plot\n\n\n::: {.cell hash='Presentation_cache/revealjs/unnamed-chunk-2_4123e0248fa036272dba52dfde21eae7'}\n::: {.cell-output-display}\n![](Presentation_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n::: {.footer}\nBlack = Mean, Yellow = 50%, Green = 70%, Purple = 90%\n:::\n\n\n## Website Generation\n\n::: {.nonincremental}\n- Rich documentation for the database\n    + <https://dynamos.mgb.org>\n    + Summary tables, figures, explanatory text\n    + Screenshots of clips, subtitles, and features\n\n- Built using [Quarto](https://quarto.org) and hosted using [GitLab Pages](https://docs.gitlab.com/ee/user/project/pages/)\n    + Can be easily updated as the database grows\n    + Can showcase the R code used to analyze data\n\n:::\n\n## Database Uses\n\n- **Emotion elicitation** video set with normative data \n\n- Affective **content analysis** with average ratings\n\n- Subjectivity analysis to predict **rating distributions**\n\n- Subjectivity analysis to explain **degree of subjectivity**\n\n- **Personalized modeling** of affective reactions\n\n## Future Directions\n\n1. Add more movie **clips** and **participants**\n\n1. Add more dynamic and holistic rating **dimensions**\n\n1. Add data from **sensors** (e.g., physiological and eye tracking)\n\n1. Collect information about participants' **personality**\n\n1. Collect similar data in other **languages/countries**\n\n1. Collect similar data in **clinical/medical** populations\n\n## Acknowledgements\n\n::: {.nonincremental}\n\n- **Co-authors**\n    + Yanmei Tie (Brigham & Women's Hospital, HMS)\n    + Einat Liebenthal (McLean Hospital, HMS)\n\n- **Funding:**\n    + Alexandra Golby (Brigham & Women's Hospital, HMS)\n\n- **Assistance:**\n    + Colin Gavin, Laura Rigolo, Abby Recko, Ben Phan\n    \n:::\n    \n## Questions? {.tc}\n\n![](movie.svg)\n\n<https://dynamos.mgb.org>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
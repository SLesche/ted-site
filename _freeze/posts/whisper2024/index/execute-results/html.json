{
  "hash": "058b1648a629486ab622d5aa8b8d2a57",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AI Transcription from R using Whisper: Part 1\"\ndescription: \"Tutorial on Using AI Transcription on Windows\"\nauthor: \"Jeffrey Girard\"\ndate: \"2024-08-14\"\nimage: whisper.webp\ndraft: false\ncategories:\n  - teaching\n  - audio\n  - AI\n---\n\n\n\n\n## Introduction\n\nIn much of my work, I study how people communicate through verbal and nonverbal behavior. To study verbal behavior, it is often necessary to generate *transcripts*, which are written records of the words that were spoken. Transcriptions can be done manually (i.e., by a person) and assisted through the use of behavioral annotation software like [ELAN](https://archive.mpi.nl/tla/elan) or [ANVIL](https://anvil-software.de) or subtitle generation and editing software like [Aegisub](https://aegisub.org/) or [Subtitld](https://www.subtitld.org/en). However, new tools based on artificial intelligence (AI) can be much more efficient and scalable, albeit at some cost to accuracy.\n\nIn this blog post, I will provide a tutorial on how to set up and use OpenAI's free [Whisper](https://openai.com/index/whisper/) algorithm to generate automatic transcriptions of audio files (either recorded originally as audio or extracted from video files). I will first show you how to quickly install the audio.whisper R package and transcribe an example file. However, the processing will be very slow and we can do much better (e.g., 10x faster) if we offload some of the work to a dedicated graphics card, such as an Nvidia card with [CUDA](https://developer.nvidia.com/about-cuda). Enabling this takes some technical work, especially on Windows, but is worth the investment if you plan to process a lot of files. This technical work will be described in Part 2.\n\n## Quickstart (easy setup, slow processing)\n\n### Install dependencies\n\nI assume you already have R (and probably an IDE like RStudio) installed. Open this up and install the development version of the \\{audio.whisper\\} package from github.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install remotes to install from github\ninstall.packages(\"remotes\") \n\n# Install audio.whisper to do transcription\nremotes::install_github(\"bnosac/audio.whisper\")\n```\n:::\n\n\n\n\n### Download whisper model\n\nLoad this new package and download one of the whisper models: `\"tiny\"`, `\"base\"`, `\"small\"`, `\"medium\"`, or `\"large-v3\"`. Earlier entries on that list are smaller (to download and hold in RAM), faster, and less accurate whereas later entries are larger, slower, and more accurate. There are also English-only versions of all but the large model, which end in `\".en\"` as in `\"base.en\"`, and may these may be more efficient if you know that all speech will be in English. You can learn more about these models via `?whisper_download_model`. For this tutorial, we will go with the `\"base\"` model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package from library\nlibrary(audio.whisper)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download or load from file the desired whisper model\nmodel <- whisper(\"base\")\n## whisper_init_from_file_with_params_no_state: loading model from 'C:/GitHub/affcomlab/posts/whisper2024/ggml-base.bin'\n## whisper_model_load: loading model\n## whisper_model_load: n_vocab       = 51865\n## whisper_model_load: n_audio_ctx   = 1500\n## whisper_model_load: n_audio_state = 512\n## whisper_model_load: n_audio_head  = 8\n## whisper_model_load: n_audio_layer = 6\n## whisper_model_load: n_text_ctx    = 448\n## whisper_model_load: n_text_state  = 512\n## whisper_model_load: n_text_head   = 8\n## whisper_model_load: n_text_layer  = 6\n## whisper_model_load: n_mels        = 80\n## whisper_model_load: ftype         = 1\n## whisper_model_load: qntvr         = 0\n## whisper_model_load: type          = 2 (base)\n## whisper_model_load: adding 1608 extra tokens\n## whisper_model_load: n_langs       = 99\n## whisper_model_load:      CPU buffer size =   147.46 MB\n## whisper_model_load: model size    =  147.37 MB\n## whisper_init_state: kv self size  =   16.52 MB\n## whisper_init_state: kv cross size =   18.43 MB\n## whisper_init_state: compute buffer (conv)   =   14.86 MB\n## whisper_init_state: compute buffer (encode) =   85.99 MB\n## whisper_init_state: compute buffer (cross)  =    4.78 MB\n## whisper_init_state: compute buffer (decode) =   96.48 MB\n```\n:::\n\n\n\n\n\n:::{.callout-note}\nNote that the larger models may take a while to download, so if you get an error that the download took longer than permitted, you can temporarily allow more time via: `options(timeout = 300)`.\n:::\n\n### Transcribe example file\n\nThe package comes with an example audio file in the proper format. Let's load it from file using `system.file()` and then transcribe it using `predict()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct file path to example audio file in package data\njfk <- system.file(package = \"audio.whisper\", \"samples\", \"jfk.wav\")\n\n# Run English transcription using the downloaded whisper model\nout <- predict(model, newdata = jfk, language = \"en\")\n## system_info: n_threads = 1 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0 | \n## Processing C:/GitHub/affcomlab/renv/library/windows/R-4.4/x86_64-w64-mingw32/audio.whisper/samples/jfk.wav (176000 samples, 11 sec), lang = en, translate = 0, timestamps = 0, beam_size = -1, best_of = 5\n## [00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n```\n:::\n\n\n\n\nThe results look good! But we can see how long this took by digging into the output object.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examine the time elapsed to process this audio\nout$timing\n## $transcription_start\n## [1] \"2024-08-14 12:47:16 CDT\"\n## \n## $transcription_end\n## [1] \"2024-08-14 13:08:06 CDT\"\n## \n## $transcription_duration\n## Time difference of 20.84114 mins\n```\n:::\n\n\n\n\nYikes, nearly 11 minutes to process just 11 seconds of audio. That's motivation to work on the CUDA version to speed things up. But before we move on to that, I'll first show you how to extract audio from a video file and convert it to the format that Whisper wants.\n\n### Extract and format audio\n\nDownload the example [mlk.mp4](mlk.mp4) video file, which is 12 seconds of Martin Luther King, Jr.'s famous \"I have a dream\" speech. This video contains an audio stream in AAC format with a sampling rate of 44.1 kHz. However, whisper requires audio files in WAV format with a sampling rate of 16 kHz. We can extract and convert it in one step using the `av_audio_convert()` function from the `av` package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install package from CRAN\n#install.packages(\"av\")\n\n# Load package from library\nlibrary(av)\n\n# Extract and convert audio\nav_audio_convert(\n  \"mlk.mp4\", \n  output = \"mlk.wav\", \n  format = \"wav\", \n  sample_rate = 16000\n)\n## [1] \"C:\\\\GitHub\\\\affcomlab\\\\posts\\\\whisper2024\\\\mlk.wav\"\n```\n:::\n\n\n\n\nNote that the process would have been identical if this had been an audio file in a different format rather than a video file - you would just replace the .mp4 file with the audio file (e.g., .mp3). Now let's transcribe this and verify that our conversion worked.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run English transcription using the downloaded whisper model\nout2 <- predict(model, newdata = \"mlk.wav\", language = \"en\")\n## system_info: n_threads = 1 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0 | \n## Processing mlk.wav (192076 samples, 12.0048 sec), lang = en, translate = 0, timestamps = 0, beam_size = -1, best_of = 5\n## [00:00:00.000 --> 00:00:02.000]   I have a dream.\n## [00:00:02.000 --> 00:00:12.000]   But one day, this nation will rise up, live up the true meaning of its creed.\n```\n:::\n\n\n\n\nNot perfect (swapped \"that\" for \"but\" and omitted an \"and\") but pretty good. And this only the base model - it might do better with a larger model, but for time's sake I'll leave that until after we get CUDA working in Part 2.\n\n*Part 2 coming soon!*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
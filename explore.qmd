---
pagetitle: "Explore | TED"
toc: true
---

## An overview of TED
```{r method-source-data-prep, child = "analysis.qmd"}
```
The following overview and analysis are a living version of the analysis conducted in TED's introductory paper. They will be updated as new data is included and may thus deviate from the published results. On this page, you can find an overview of included data, a brief meta-analysis on the truth effect within TED, and additional models estimating variability in the truth effect on subject, statement, or experiment level.

In the current version of TED, we included `r nrow(study_overview) %>% apa_num()` studies from `r nrow(publications_overview) %>% apa_num()` publications, spanning `r length(unique(full_data$subject)) %>% apa_num()` participants contributing `r nrow(full_data) %>% apa_num()` trials. A complete list of the included publications can be found in the Table "*overview of studies included in TED*".

```{r, echo = FALSE}
age_data <- study_overview %>% 
  mutate(participant_age = ifelse(participant_age == 99, NA, participant_age)) %>% 
  filter(!is.na(participant_age), !is.na(n_participants))
```

Sample composition ranged from `r min(study_overview$n_participants, na.rm = TRUE)` to `r max(study_overview$n_participants, na.rm = TRUE)` participants. On average, studies included `r mean(study_overview$n_participants, na.rm = TRUE) %>% apa_num()` participants ($\mu_{age} =$ `r weighted.mean(age_data$participant_age, age_data$n_participants, na.rm = TRUE) %>% apa_num()`,$\sigma_{age} =$ `r DescTools::SD(age_data$participant_age, weights = age_data$n_participants, na.rm = TRUE) %>% apa_num()`). An overview of the rating scale usage for truth judgments and the use of a filler task over all included studies can be found in the figure below.

```{r study-overview-plot, echo = FALSE}
#| out-width: "100%"
#| label: study-overview-plot
#| fig-cap: "Overview of Study-related variables in TED"
knitr::include_graphics("img/study_overview_plot.png")
```

On average, studies employed `r mean(procedure_data$n_statements) %>% apa_num()` ($SD =$ `r sd(procedure_data$n_statements) %>% apa_num()`) statements per 
participant in the judgment session and in `r print_freq_percent(sum(procedure_data$percent_repeated == 50) / nrow(procedure_data))` of procedure settings exactly 50% of statements were repeated. Of `r nrow(procedure_data)` judgment phases, `r print_freq_percent(sum(procedure_data$repetition_time < 180) / nrow(procedure_data))` were conducted on the same day as the exposure phase. The average delay between exposure and judgment phase if both were conducted on the same day was `r procedure_data %>% filter(repetition_time < 180) %>% pull(repetition_time) %>% mean() %>% apa_num()` minutes. The average delay between exposure and judgment phase, given the judgment phase was conducted at least one day after the exposure phase, was `r round((procedure_data %>% filter(repetition_time > 180) %>% pull(repetition_time) %>% mean())/(60*24), 2)` days. An overview of additional variables pertaining to the procedure of the included studies can be found in the Figure below.

<!-- An overview of the delay between exposure and judgment phase is provided in Figure \@ref(fig:delay-overview-plot).  -->

<!-- (ref:delay-overview-plot) Overview of delay between exposure and test phase -->
<!-- ```{r delay-overview-plot, fig.cap = paste("(ref:delay-overview-plot)"), out.width="100%"} -->
<!-- knitr::include_graphics("images/delay_overview_plot.png") -->
<!-- ``` -->

```{r procedure-overview-plot, echo = FALSE}
#| out-width: "100%"
#| label: procedure-overview-plot
#| fig-cap: "Overview of Procedure-related variables in TED"
knitr::include_graphics("img/procedure_overview_plot.png")
```

Detailed information on the statements presented is available for `r study_overview %>% filter(statementset_id != 1) %>% nrow()` out of `r study_overview %>% nrow()` studies. Data on the accuracy of a statement is available for `r length(analysis_data$statement_accuracy[which(!is.na(analysis_data$statement_accuracy))]) %>% apa_num` (`r (length(analysis_data$statement_accuracy[which(!is.na(analysis_data$statement_accuracy))]) / nrow(analysis_data)) %>% print_freq_percent()`) of trials, the exact statement text is available for `r length(analysis_data$statement_text[which(!is.na(analysis_data$statement_text))]) %>% apa_num` (`r (length(analysis_data$statement_text[which(!is.na(analysis_data$statement_text))]) / nrow(analysis_data)) %>% print_freq_percent()`) of trials, and response times are available for `r length(analysis_data$rt[which(!is.na(analysis_data$rt))]) %>% apa_num` (`r (length(analysis_data$rt[which(!is.na(analysis_data$rt))]) / nrow(analysis_data)) %>% print_freq_percent()`) of trials.

```{r, echo = FALSE, message=FALSE, warning = FALSE}
library(knitr)
library(kableExtra)
library(effsize)
library(metafor)
table_publication_overview <- procedure_data %>% 
  left_join(study_overview) %>% 
  left_join(publications_overview) %>% 
  select(publication_id, study_id, procedure_id,
         n_participants, student_sample, truth_rating_steps,
         repetition_time, n_statements
         )

kable(
  table_publication_overview,
  caption = "Overview of studies included in TED"
) %>%
  scroll_box(width = "100%", height = "400px") %>%
  add_footnote(label = "A note goes here.") %>% 
  kable_styling(fixed_thead = T)

```

## Meta-Analysis

The following provides an **illustrative meta-analysis** of effect sizes derived from the TED Truth Effect database. It is based on trial-level data and demonstrates how a meta-analysis could be conducted. **This example is not a definitive guide**, nor does TED represent a comprehensive or random sample of all studies, since it only includes studies with openly available trial-level data.

Here, we included only studies with a heterogeneous presentation criterion ("between-items criterion"; Dechene et al., 2010). Effect sizes were calculated using **Hedges' g**, derived as follows:

1. For each subject within a study, the repeated and new average responses were calculated.
2. Hedges' g was computed per study using the `effsize::cohen.d()` function with the paired correction.
3. Variances of the effect sizes were extracted to serve as input for the meta-analysis.
4. The meta-analysis accounts for multiple entries per publication (as some publications have multiple studies).

Some small deviations from the originally reported effect sizes in the individual studies may exist. But we applied no additional exclusion criteria and tried to exclude (during encoding) all subjects excluded in the original studies.

First, we access the database and retrieve trial-level data:

```{r, eval = FALSE, echo = TRUE}
library(acdcquery)

# Optionally Download current version of TED
# download_ted("path/to/")

# Replace with your local path
conn <- connect_to_db("path/to/ted.db")

analysis_data <- query_db(
    conn,
    arguments,
    target_vars = c("default", "study_id", "publication_id", "authors", "conducted"),
    target_table = "observation_table"
  ) %>% 
  filter(phase == "test") %>% 
  filter(!is.na(repeated), !is.na(response)) 

# Here we only use data where the test phase has both 
# repeated and new statements
has_complete_data <- analysis_data %>% 
  count(procedure_id, subject, repeated) %>% 
  count(procedure_id, subject) %>% 
  mutate(
    has_complete_data = ifelse(n == 2, 1, 0)
  )

analysis_data <- analysis_data %>% 
  left_join(
    ., has_complete_data
  ) %>% 
  filter(has_complete_data == 1) 
```

Then we compute effect sizes per study using `cohen.d`.

```{r, echo = TRUE}
eff_data <- analysis_data %>% 
  left_join(publications_overview) %>% 
  group_by(publication_id, authors, conducted, study_id, repeated, subject) %>% 
  summarize(
    mean_resp = mean(response, na.rm = TRUE)
  ) %>% 
  mutate(repeated = factor(
    ifelse(repeated > 0, "yes", "no"),
    levels = c("yes", "no"))
    ) %>% 
  pivot_wider(names_from = repeated, values_from = mean_resp) %>% 
  group_by(publication_id, authors, conducted, study_id) %>% 
  nest() %>% 
  mutate(effsize = map(
    data, 
    ~effsize::cohen.d(
      .$yes, 
      .$no, 
      data = ., 
      hedges.correction = TRUE,
      paired = TRUE
      )
    )
  ) %>% 
  mutate(
    estimate = map_dbl(effsize, ~{.$estimate}),
    var = map_dbl(effsize, ~{.$var})
  )
```
```{r, echo = FALSE}
#' Calculate I-squared values and variance distribution for multilevel meta-analysis models
#'
#' This function calculates values of \eqn{I^2} and the variance distribution for multilevel meta-analysis
#' models fitted with \code{\link[metafor]{rma.mv}}.
#'
#'
#' @usage mlm.variance.distribution(x)
#'
#' @param x An object of class \code{rma.mv}. Must be a multilevel model with two random effects (three-level meta-analysis model).
#'
#' @details This function estimates the distribution of variance in a three-level meta-analysis
#' model (fitted with the \code{\link[metafor]{rma.mv}} function). The share of variance attributable to
#' sampling error, within and between-cluster heterogeneity is calculated,
#' and an estimate of \eqn{I^2} (total and for Level 2 and Level 3) is provided. The function uses the formula by
#' Cheung (2014) to estimate the variance proportions attributable to each model component and to derive the \eqn{I^2} estimates.
#'
#'
#' @references
#'
#' Harrer, M., Cuijpers, P., Furukawa, T.A, & Ebert, D. D. (2019).
#' \emph{Doing Meta-Analysis in R: A Hands-on Guide}. DOI: 10.5281/zenodo.2551803. \href{https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/mlma.html}{Chapter 12}.
#'
#'Cheung, M. W. L. (2014). Modeling dependent effect sizes with three-level meta-analyses: a structural equation modeling approach. \emph{Psychological Methods, 19}(2), 211.
#'
#' @author Mathias Harrer & David Daniel Ebert
#'
#' @aliases var.comp
#'
#' @import ggplot2
#' @importFrom stats model.matrix
#'
#' @return Returns a data frame containing the results. A plot summarizing the variance distribution and \eqn{I^2} values can be generated using \code{plot}.
#'
#' @export mlm.variance.distribution
#' @export var.comp
#'
#' @examples
#' # Use dat.konstantopoulos2011 from the "metafor" package
#' library(metafor)
#'
#' # Build Multilevel Model (Three Levels)
#' m = rma.mv(yi, vi, random = ~ 1 | district/school, data=dat.konstantopoulos2011)
#'
#' # Calculate Variance Distribution
#' mlm.variance.distribution(m)
#'
#' # Use alias 'var.comp' and 'Chernobyl' data set
#' data("Chernobyl")
#' m2 = rma.mv(yi = z, V = var.z, data = Chernobyl, random = ~ 1 | author/es.id)
#' res = var.comp(m2)
#'
#' # Print results
#' res
#'
#' # Generate plot
#' plot(res)



mlm.variance.distribution = var.comp = function(x){

  m = x

  # Check class
  if (!(class(m)[1] %in% c("rma.mv", "rma"))){
    stop("x must be of class 'rma.mv'.")
  }

  # Check for three level model
  if (m$sigma2s != 2){
    stop("The model you provided does not seem to be a three-level model. This function can only be used for three-level models.")
  }

  # Check for right specification (nested model)
  if (sum(grepl("/", as.character(m$random[[1]]))) < 1){
    stop("Model must contain nested random effects. Did you use the '~ 1 | cluster/effect-within-cluster' notation in 'random'? See ?metafor::rma.mv for more details.")
  }

  # Get variance diagonal and calculate total variance
  n = m$k.eff
  vector.inv.var = 1/(diag(m$V))
  sum.inv.var = sum(vector.inv.var)
  sum.sq.inv.var = (sum.inv.var)^2
  vector.inv.var.sq = 1/(diag(m$V)^2)
  sum.inv.var.sq = sum(vector.inv.var.sq)
  num = (n-1)*sum.inv.var
  den = sum.sq.inv.var - sum.inv.var.sq
  est.samp.var = num/den

  # Calculate variance proportions
  level1=((est.samp.var)/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
  level2=((m$sigma2[2])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
  level3=((m$sigma2[1])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)

  # Prepare df for return
  Level=c("Level 1", "Level 2", "Level 3")
  Variance=c(level1, level2, level3)
  df.res=data.frame(Variance)
  colnames(df.res) = c("% of total variance")
  rownames(df.res) = Level
  I2 = c("---", round(Variance[2:3], 2))
  df.res = as.data.frame(cbind(df.res, I2))

  totalI2 = Variance[2] + Variance[3]


  # Generate plot
  df1 = data.frame("Level" = c("Sampling Error", "Total Heterogeneity"),
                  "Variance" = c(df.res[1,1], df.res[2,1]+df.res[3,1]),
                  "Type" = rep(1,2))

  df2 = data.frame("Level" = rownames(df.res),
                   "Variance" = df.res[,1],
                   "Type" = rep(2,3))

  df = as.data.frame(rbind(df1, df2))


  g = ggplot(df, aes(fill=Level, y=Variance, x=as.factor(Type))) +
    coord_cartesian(ylim = c(0,1), clip = "off") +
    geom_bar(stat="identity", position="fill", width = 1, color="black") +
    scale_y_continuous(labels = scales::percent)+
    theme(axis.title.x=element_blank(),
          axis.text.y = element_text(color="black"),
          axis.line.y = element_blank(),
          axis.title.y=element_blank(),
          axis.line.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.y = element_line(lineend = "round"),
          legend.position = "none",
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          legend.background = element_rect(linetype="solid",
                                           colour ="black"),
          legend.title = element_blank(),
          legend.key.size = unit(0.75,"cm"),
          axis.ticks.length=unit(.25, "cm"),
          plot.margin = unit(c(1,3,1,1), "lines")) +
    scale_fill_manual(values = c("darkseagreen3", "deepskyblue3", "darkseagreen2",
                                 "deepskyblue1", "deepskyblue2")) +

    # Add Annotation

    # Total Variance
    annotate("text", x = 1.5, y = 1.05,
             label = paste("Total Variance:",
                           round(m$sigma2[1]+m$sigma2[2]+est.samp.var, 3))) +

    # Sampling Error
    annotate("text", x = 1, y = (df[1,2]/2+df[2,2])/100,
             label = paste("Sampling Error Variance: \n", round(est.samp.var, 3)), size = 3) +

    # Total I2
    annotate("text", x = 1, y = ((df[2,2])/100)/2-0.02,
             label = bquote("Total"~italic(I)^2*":"~.(round(df[2,2],2))*"%"), size = 3) +
    annotate("text", x = 1, y = ((df[2,2])/100)/2+0.05,
             label = paste("Variance not attributable \n to sampling error: \n", round(m$sigma2[1]+m$sigma2[2],3)), size = 3) +

    # Level 1
    annotate("text", x = 2, y = (df[1,2]/2+df[2,2])/100, label = paste("Level 1: \n",
                                                                       round(df$Variance[3],2), "%", sep=""), size = 3) +

    # Level 2
    annotate("text", x = 2, y = (df[5,2]+(df[4,2]/2))/100,
             label = bquote(italic(I)[Level2]^2*":"~.(round(df[4,2],2))*"%"), size = 3) +

    # Level 3
    annotate("text", x = 2, y = (df[5,2]/2)/100,
             label = bquote(italic(I)[Level3]^2*":"~.(round(df[5,2],2))*"%"), size = 3)

  returnlist = list(results = df.res,
                    totalI2 = totalI2,
                    plot = g)
  class(returnlist) = c("mlm.variance.distribution", "list")

  invisible(returnlist)

  returnlist

}


```

### Multi-Level Meta-Analysis

To account for non-independence of effect sizes within publications contributing multiple studies, we fitted a three-level meta-analytic model using rma.mv():

Level 1: Sampling variance of individual effect sizes

Level 2: Heterogeneity between studies within the same publication

Level 3: Heterogeneity between publications

```{r, echo = TRUE}
eff_data <- eff_data %>% 
  mutate(label = paste0(authors, " (", conducted, ")"))

res_mv <- rma.mv(yi = estimate,
                 V = var,
                 random = ~ 1 | publication_id/study_id,  # random intercept per study
                 slab = label,
                 data = eff_data,
                 method = "REML")
summary(res_mv)
```
The results reveal a large effect of repetition ($d =$ `r res_mv$beta[1, 1] %>% apa_num()`; 95 % CI = `r res_mv$ci.lb %>% apa_num()`, `r res_mv$ci.ub %>% apa_num()`). This is much larger than the effect size found in Dechene et al. (2010) of around $d = 0.49$ for the between-items criterion. Notably, this analysis is only based on publications with openly available data. Thus, these results are based on a biased smaller sample of k = `r res_mv$data$publication_id %>% unique() %>% length()` publications. 

### Variance Decomposition

We can calculate variance proportions and I² values to quantify the contributions of sampling error, within-publication heterogeneity, and between-publication heterogeneity. This reveals substantial variance both within a publication and between publications, supporting the use of this multi-level approach.

```{r, echo = TRUE}
i2 <- var.comp(res_mv)
i2$plot
```

### Forest Plot

Finally, a forest plot shows the individual study effect sizes, their confidence intervals, and the overall estimate from the multi-level meta-analysis model.
```{r, echo = TRUE}
# forest plot for multi-level model
forest(res_mv, 
       slab = paste0(eff_data$authors, " (", eff_data$conducted, ") | Study: ", eff_data$study_id),
       xlab = "Effect size (Hedges' g)",
       refline = 0,
       cex = 0.5,
      ) 
```

### Summary

This workflow demonstrates how the TED Truth Effect database can be used to:

- Compute effect sizes at the trial and study level

- Fit multi-level meta-analytic models to account for clustering

- Explore variance components and heterogeneity

- Visualize results using a forest plot

It is intended as a tutorial example and not a definitive meta-analysis.

## Hierachical Bayesian Model

To illustrate the benefits of our large collection of trial-level data, we fitted Bayesian multilevel models predicting truth judgments, with repetition as a fixed effect and random intercepts and slopes at the subject, statement, and procedure levels.

The experimental level is based on data at the level of the procedure_table, as this table contains detailed information about each experimental setup (e.g., proportion of repeated items, presence of warnings, number of sessions) beyond what is available in the broader study_table. Each entry in the study_table corresponds to at least one entry in the procedure_table, but a single study may include several procedures that differ in these settings. For example, the same study may have multiple judgment sessions, modify the percentage of repeated stimuli, or warn some participants about the truth effect. These different procedures will then also receive different procedure identifiers, but the same study identifier.

Thus, the experiment identifier (procedure_id) uniquely captures both the study context and its specific experimental conditions. This modeling approach allows us to estimate the variance in the truth effect at three levels simultaneously: (1) variance due to diverse statements (statement level), (2) variance due to individual differences (subject level), and (3) variance due to common experimental manipulations and study settings (experiment level).

We analyzed the dichotomous and Likert-type response formats separately due to differences in their scale characteristics. Dichotomous responses (e.g., true/false) require logistic models, whereas Likert-type responses (e.g., 1–5 ratings) allow for linear models. All responses were maximum-normalized to the range 0-1 with one representing the maximum possible response indicating a "true" judgment. The repetition status was mean-centered to aid model estimation, a new statement was coded -0.5 and a repeated statement 0.5.

We ran all models using 4 chains with 3000 iterations per chain, 1000 of which were discarded as warmup-samples, leading to a total of 8000 posterior samples. There were no divergent transitions, no $\hat{R} > 1.05$, and visual inspection confirmed that the chains mixed well. We used weakly informative priors for the intercept, fixed effect, and standard deviations for all models.

$$Intercept \sim Normal(0.5, 0.5)$$
$$b \sim Normal(0, 1)$$
$$\sigma \sim Gamma(1, 4)$$

### Dichotomous Truth Judgments
The analysis was based on `r nrow(dichotomous_data) %>% apa_num()` trials nested within `r length(unique(dichotomous_data$subject)) %>% apa_num()` subjects, `r length(unique(dichotomous_data$statement_id)) %>% apa_num()` statements, and `r length(unique(dichotomous_data$procedure_id)) %>% apa_num()` experiments.

The table below provides a summary of parameter estimates. As expected, the model indicated a significant fixed effect of repetition ($b =$ `r summary_model_full_scale_bayes$fixed$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_dichotomous_bayes$fixed[2, 3] %>% apa_num(), ", ", summary_model_full_dichotomous_bayes$fixed[2, 4] %>% apa_num(), "]")`, $OR =$ `r summary_model_full_dichotomous_bayes$fixed$Estimate[2] %>% exp() %>% apa_num()`, $BF_{10} > 1 \times 10^{10}$). Notably, the standard deviation of the random slope of repetition was highest at the subject level ($\sigma =$ `r summary_model_full_dichotomous_bayes$random$subject$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_dichotomous_bayes$random$subject[2, 3] %>% apa_num(), ", ", summary_model_full_dichotomous_bayes$random$subject[2, 4] %>% apa_num(), "]")`), followed by the experiment level ($\sigma =$ `r summary_model_full_dichotomous_bayes$random$procedure_id$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_dichotomous_bayes$random$procedure_id[2, 3] %>% apa_num(), ", ", summary_model_full_dichotomous_bayes$random$procedure_id[2, 4] %>% apa_num(), "]")`), and the statement level ($\sigma =$ `r summary_model_full_dichotomous_bayes$random$statement_id$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_dichotomous_bayes$random$statement_id[2, 3] %>% apa_num(), ", ", summary_model_full_dichotomous_bayes$random$statement_id[2, 4] %>% apa_num(), "]")`).

```{r dichotomous-model-table, echo = FALSE}
#| label: dichotomous-model-table
#| tbl-cap: "Variance in the truth effect at different levels"
dichomotous_model_table <- summary_model_full_dichotomous_bayes$fixed %>% 
  mutate(Effect = "fixed",
         Parameter = row.names(.),
         Grouping = "") %>% 
  select(Effect, Grouping, Parameter, Estimate, `l-95% CI`, `u-95% CI`)

dichomotous_model_table <- rbind(dichomotous_model_table, 
                                 summary_model_full_dichotomous_bayes$random %>% 
                                   data.table::rbindlist() %>% 
                                   mutate(Grouping = c("experiment", "experiment", "statement", "statement", "subject", "subject")) %>% 
                                   mutate(Effect = "random",
                                          Parameter = rep(c("Intercept (sd)", "repeated (sd)"), 3)) %>% 
                                   select(Effect, Grouping, Parameter, Estimate, `l-95% CI`, `u-95% CI`)
) %>% 
  remove_rownames() %>% 
  rename("l_95_CrI" = "l-95% CI",
         "u_95_CrI" = "u-95% CI")

apa_table(dichomotous_model_table, 
          caption = "Parameter Estimates of the Dichotomous Model", 
          note = paste0("N = ", summary_model_full_dichotomous_bayes$nobs,
                        "; N Experiments = ", summary_model_full_dichotomous_bayes$ngrps$procedure_id,
                        "; N Subjects = ", summary_model_full_dichotomous_bayes$ngrps$subject,
                        "; N Statements = ", summary_model_full_dichotomous_bayes$ngrps$statement_id,
                        "; l_95_CrI refers to the lower boundary of the 95% credible interval, u_95_CrI refers to the upper boundary"))
```

```{r, eval = FALSE, echo = FALSE}
draws_array <- as_draws_array(model_full_dichotomous_bayes)

color_scheme_set("red")
mcmc_areas(draws_array, 
           pars = c("sd_subject__repeated", "sd_procedure_id__repeated", "sd_statement_id__repeated"),
           prob = 0.95)
```

### Scale Truth Judgments
The analysis was based on `r nrow(scale_data) %>% apa_num()` trials nested within `r length(unique(scale_data$subject)) %>% apa_num()` subjects, `r length(unique(scale_data$statement_id)) %>% apa_num()` statements, and `r length(unique(scale_data$procedure_id)) %>% apa_num()` experiments.

The table below provides a summary of parameter estimates. As expected, the model indicated a significant fixed effect of repetition ($b =$ `r summary_model_full_scale_bayes$fixed$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_scale_bayes$fixed[2, 3] %>% apa_num(), ", ", summary_model_full_scale_bayes$fixed[2, 4] %>% apa_num(), "]")`, $BF_{10} > 1 \times 10^{10}$). Again, the standard deviation of the random slope of repetition was highest at the subject level ($\sigma =$ `r summary_model_full_scale_bayes$random$subject$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_scale_bayes$random$subject[2, 3] %>% apa_num(), ", ", summary_model_full_scale_bayes$random$subject[2, 4] %>% apa_num(), "]")`), followed by the experiment level ($\sigma =$ `r summary_model_full_scale_bayes$random$procedure_id$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_scale_bayes$random$procedure_id[2, 3] %>% apa_num(), ", ", summary_model_full_scale_bayes$random$procedure_id[2, 4] %>% apa_num(), "]")`), and the statement level ($\sigma =$ `r summary_model_full_scale_bayes$random$statement_id$Estimate[2] %>% apa_num()`, $95\% \ CrI =$ `r paste0("[", summary_model_full_scale_bayes$random$statement_id[2, 3] %>% apa_num(), ", ", summary_model_full_scale_bayes$random$statement_id[2, 4] %>% apa_num(), "]")`).

```{r scale-model-table, echo = FALSE}
#| label: scale-model-table
#| tbl-cap: "Variance in the truth effect at different levels"
scale_model_table <- summary_model_full_scale_bayes$fixed %>% 
  mutate(Effect = "fixed",
         Parameter = row.names(.),
         Grouping = "") %>% 
  select(Effect, Grouping, Parameter, Estimate, `l-95% CI`, `u-95% CI`)

scale_model_table <- rbind(scale_model_table, 
                                 summary_model_full_scale_bayes$random %>% 
                                   data.table::rbindlist() %>% 
                                   mutate(Grouping = c("experiment", "experiment", "statement", "statement", "subject", "subject")) %>% 
                                   mutate(Effect = "random",
                                          Parameter = rep(c("Intercept (sd)", "repeated (sd)"), 3)) %>% 
                                   select(Effect, Grouping, Parameter, Estimate, `l-95% CI`, `u-95% CI`)
) %>% 
  remove_rownames() %>% 
  rename("l_95_CrI" = "l-95% CI",
         "u_95_CrI" = "u-95% CI")

apa_table(scale_model_table, caption = "Parameter Estimates of the Scale Model", 
          note = paste0("N = ", summary_model_full_scale_bayes$nobs,
                        "; N Experiments = ", summary_model_full_scale_bayes$ngrps$procedure_id,
                        "; N Subjects = ", summary_model_full_scale_bayes$ngrps$subject,
                        "; N Statements = ", summary_model_full_scale_bayes$ngrps$statement_id,
                        "; l_95_CrI refers to the lower boundary of the 95% credible interval, u_95_CrI refers to the upper boundary"))
```

```{r, eval = FALSE, echo = FALSE}
draws_array <- as_draws_array(model_full_scale_bayes)

color_scheme_set("red")
mcmc_areas(draws_array, 
           pars = c("sd_subject__repeated", "sd_procedure_id__repeated", "sd_statement_id__repeated"),
           prob = 0.95)
```

To further explore the influence of temporal delay between the exposure and judgment phases on inter-individual variability in the repetition effect, we included an interaction between subject and temporal delay (same-day vs. different day) in the random effect structure. The model then estimates two standard distributions for the random effect of repetition on the subject level. We can then investigate whether the difference in the standard deviation of the random effect of repetition on the subject-level is different depending on the temporal delay.

The table below provides a summary of parameter estimates. The standard deviation of the random slope of repetition at the subject level for a same-day judgment phase was $\sigma_0 =$ `r mean(sd_1) %>% apa_num()` ($95\% \ CrI =$ `r paste0("[", quantile(sd_1, 0.025) %>% apa_num(), ", ", quantile(sd_1, 0.975) %>% apa_num(), "]")`). The standard deviation for the random slope on a later day judgment phase was $\sigma_1 =$ `r mean(sd_0) %>% apa_num()` ($95\% \ CrI =$ `r paste0("[", quantile(sd_0, 0.025) %>% apa_num(), ", ", quantile(sd_0, 0.975) %>% apa_num(), "]")`). The difference in standard deviations in the random effect of repetition at the subject level deviated substantially from zero $\sigma_0 - \sigma_1 =$ `r mean(sd_diff) %>% apa_num()` ($95\% \ CrI =$ `r paste0("[", quantile(sd_diff, 0.025) %>% apa_num(), ", ", quantile(sd_diff, 0.975) %>% apa_num(), "]")`, $BF_{10} > 1 \times 10^{10}$).

```{r time-model-table, echo = FALSE}
#| label: time-model-table
#| tbl-cap: "Variance in the truth effect at different levels"

time_model_table <- summary_model_time_scale_bayes$fixed %>% 
  mutate(Effect = "fixed",
         Parameter = row.names(.),
         Grouping = "") %>% 
  select(Effect, Grouping, Parameter, Estimate, `l-95% CI`, `u-95% CI`)

time_model_table <- rbind(time_model_table, 
                                 summary_model_time_scale_bayes$random %>% 
                                   data.table::rbindlist() %>% 
                                   mutate(Grouping = c("experiment", "experiment", "statement", "statement", "subject", "subject")) %>% 
                                   mutate(Effect = "random",
                                          Parameter = rep(c("Intercept (sd)", "repeated (sd)"), 3)) %>% 
                                   select(Effect, Grouping, Parameter, Estimate, `l-95% CI`, `u-95% CI`) %>% 
                            filter(Grouping != "subject")
) %>% 
  remove_rownames() %>% 
  rename("l_95_CrI" = "l-95% CI",
         "u_95_CrI" = "u-95% CI")

time_model_table <- rbind(time_model_table,
                          data.frame(
                            Effect = rep("random", 4),
                            Grouping = c("subject (same day)", "subject (same day)", "subject (later)", "subject (later)"),
                            Parameter = rep(c("Intercept (sd)", "repeated (sd)"), 2),
                            Estimate = c(mean(intercept_1), mean(sd_1), mean(intercept_0), mean(sd_0)),
                            `l_95_CrI` = c(quantile(intercept_1, 0.025), quantile(sd_1, 0.025), quantile(intercept_0, 0.025), quantile(sd_0, 0.025)),
                            `u_95_CrI` = c(quantile(intercept_1, 0.975), quantile(sd_1, 0.975), quantile(intercept_0, 0.975), quantile(sd_0, 0.975))
                          ))

apa_table(time_model_table, caption = "Parameter Estimates of the Time-based Scale Model", 
          note = paste0("N = ", summary_model_time_scale_bayes$nobs,
                        "; N Experiments = ", summary_model_time_scale_bayes$ngrps$procedure_id,
                        "; N Subjects = ", summary_model_time_scale_bayes$ngrps$subject,
                        "; N Statements = ", summary_model_time_scale_bayes$ngrps$statement_id,
                        "; l_95_CrI refers to the lower boundary of the 95% credible interval, u_95_CrI refers to the upper boundary"))
```

```{r time-model-plot, echo = FALSE}
#| out-width: "100%"
#| label: time-model-plot
#| fig-cap:  "Variance in the truth effect at different levels"
post <- as_draws_df(model_time_scale_bayes)

sd_statement <- post$sd_statement_id__repeated
sd_procedure <- post$sd_procedure_id__repeated

sd_matrix <- cbind(
  sd_1,
  sd_0,
  sd_procedure,
  sd_statement
)
color_scheme_set("red")

sd_posterior_plot <- mcmc_areas(
  sd_matrix,
  prob = 0.95,      # 95% credible interval
  prob_outer = 1,
  border_size = 1,
  point_est = "median") +
  xlim(0, 0.12)+
  xlab("Standard Deviation") +
  # ylab("Level") +
  scale_y_discrete(
    labels = c(
      "sd_1"= "Subject (retention interval < 1 day)",
      "sd_0" = "Subject (retention interval ≥ 1 day)",
      "sd_procedure" = "Procedure",
      "sd_statement" = "Statement"
    )
  )

sd_posterior_plot
```
